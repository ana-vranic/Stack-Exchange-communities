{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1882331b-d8bb-487d-a8aa-2f3c579d12e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import h5py\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5e4b9a0-9dca-4559-a5c0-12f89e068373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data location\n",
    "data_path = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b528f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_coefficient(x):\n",
    "    \"\"\"Compute Gini coefficient of array of values x\"\"\"\n",
    "    diffsum = 0\n",
    "    for i, xi in enumerate(x[:-1], 1):\n",
    "        diffsum += np.sum(np.abs(xi - x[i:]))\n",
    "    return diffsum / (len(x)**2 * np.mean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "322f4e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dyn_rep_aggregates(community,save_path):\n",
    "    '''\n",
    "    Aggregation of dynamical reputation within community.\n",
    "    Input community (SE website) name and save_path for resulting dataframe.\n",
    "    Output dataframe containing following columns:\n",
    "    - Day - integer days, corresponding to columns of dynamical reputation input file\n",
    "    - Number of active users - users with reputation above some threshold, default 1\n",
    "    - Mean user reputation - among active users\n",
    "    - Gini coefficient - among all users\n",
    "    - Gini coefficient active users - among active users only\n",
    "    '''\n",
    "    \n",
    "    # reputation data load\n",
    "    reputation_path = data_path+'reputations/'\n",
    "    reputation_file_name = '%s_first_180_days_eng_reputation.csv'%(community)\n",
    "    rep_data = pd.read_csv(reputation_path+reputation_file_name,index_col=0)\n",
    "\n",
    "    # dynamical reputation threshold value for user to be called active\n",
    "    threshold = 1\n",
    "    \n",
    "    dr_df = pd.DataFrame(columns=['Day','Number of active users','Mean user reputation','Gini coefficient', 'Gini coeficient active users'])\n",
    "    \n",
    "    # number of users with reputation above some threshold\n",
    "    dr_df['Number of active users'] = (rep_data>threshold).sum(axis=0)\n",
    "    # mean user reputation only among active users\n",
    "    dr_df['Mean user reputation'] = rep_data[rep_data>threshold].mean()\n",
    "    # gini coefficient among users' reputations\n",
    "    dr_df['Gini coefficient'] = rep_data.apply(gini_coefficient,axis=0)\n",
    "    # gini coeffieicent based only on active users' reputation\n",
    "    dr_df['Gini coeficient active users'] = rep_data[rep_data>threshold].apply(gini_coefficient,axis=0)\n",
    "    dr_df['Day'] = rep_data.columns.astype(int)\n",
    "\n",
    "    filename = '%s_dynamic_reputation_aggregates.csv'%(community)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    dr_df.to_csv(save_path+filename,index=False)\n",
    "    return dr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb94e67b",
   "metadata": {},
   "source": [
    "We run this function for all four pairs of communities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f050c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "communities = ['astronomy','economics','literature','physics',\n",
    "               '052012astronomy','052012economics','052012-literature','052012-theoretical-physics']\n",
    "for comm in communities:\n",
    "    dyn_rep_aggregates(comm, data_path+'processed data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b94ba934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_questions_within_window(community,save_path,twin=7):\n",
    "    \"\"\"\n",
    "    This function counts active questions (those with an answer, comment or accepted answer) within a time window specified by twin.\n",
    "    Default is weekly active question count.\n",
    "    Iput:\n",
    "    - net: can be 'launched' or 'area51'\n",
    "    - community: name of SE site, together with net it is used to get appropriate input files that contain reputations and interactions\n",
    "    - save_path: path to directory where output dataframe should be saved\n",
    "    - twin: time window within which interactions are aggregated, default is 7 days\n",
    "    Output:\n",
    "    - if save_path is not provided dataframe is returned, otherwise it is saved at save_path location\n",
    "    DataFrame contains columns\n",
    "    - First day of twin sliding window\n",
    "    - Number of active questions\n",
    "    \"\"\"\n",
    "    \n",
    "    # import interactions via questions, answers and comments for network:\n",
    "    interactions_path = data_path+'/interactions/'\n",
    "    qa = pd.read_csv(interactions_path + '%s/%s_interactions_questions_answers.csv'%(community,community))\n",
    "    comm = pd.read_csv(interactions_path + '%s/%s_interactions_comments.csv'%(community,community))\n",
    "    acc = pd.read_csv(interactions_path + '%s/%s_interactions_acc_answers.csv'%(community,community))\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for t in range(180-twin):\n",
    "        temp_data = []\n",
    "        temp_data.append(t)\n",
    "        \n",
    "        qnaT = qa[(qa['days']>=t)&(qa['days']<t+twin)].dropna().copy()\n",
    "        commT = comm[(comm['days']>=t)&(comm['days']<t+twin)].dropna().copy()\n",
    "        accT = acc[(acc['days']>=t)&(acc['days']<t+twin)].dropna().copy()\n",
    "        \n",
    "        numbQ = len(set(qnaT.QId).union(set(commT.QId)).union(set(accT.QId)))\n",
    "        temp_data.append(numbQ)\n",
    "\n",
    "        data.append(temp_data)\n",
    "    \n",
    "    data_df = pd.DataFrame(data, columns = ['FirstDay','Number_of_active_questions'])\n",
    "    if save_path:    \n",
    "        filename = '%s_weekly_active_questions.csv'%(community)\n",
    "        os.makedirs(save_path, exist_ok = True)\n",
    "        data_df.to_csv(save_path+filename,index=False)\n",
    "    else:    \n",
    "        return(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6be05117",
   "metadata": {},
   "outputs": [],
   "source": [
    "communities = ['astronomy','economics','literature','physics',\n",
    "               '052012astronomy','052012economics','052012-literature','052012-theoretical-physics']\n",
    "for comm in communities:\n",
    "    active_questions_within_window(comm, data_path+'processed data/active questions/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a08e664b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_network_and_dyn_reputation_features(net,community,save_path,twin=30):\n",
    "    '''\n",
    "    This function constructs undirected unweighted network based on question, answer and comment interactions.\n",
    "    Some network features are extracted and correlated with dynamical reputation. Resulting dataframe contains \n",
    "    features for first 180 days split into 150 30day sliding windows.\n",
    "    Inputs:\n",
    "    - net: can be \"launched\" or \"area51\"\n",
    "    - community: name of SE site, together with net it is used to get appropriate input files that contain reputations and interactions\n",
    "    - save_path: path to directory where output dataframe should be saved\n",
    "    - twin: time window within which interactions are aggregated, default is 30 days\n",
    "    Outputs:\n",
    "    - if save_path is not provided ('') dataframe is returned, otherwise it is saved\n",
    "    DataFrame contains following columns\n",
    "    - First day of twin sliding window\n",
    "    - Clustering_coef - average clustering coeficent in the interaction network\n",
    "    - DynRep_Degree_corr - correlation coeficient between user's degree in the interaction network and their dynamical reputation at the last day in twin\n",
    "    - DynRep_BC_corr - as above but instead of degree, node betweenness centrality is used\n",
    "    - DynRep_Assortativity - assortativity between users dynamical reputations within the interaction network (e.g. for each network's edge correlation between nodes' dynamical reputations is calculated) \n",
    "    high positive assortativity means that users connect with users of similar reputation, high negative assortativity means users with high reputation mainly interact with users with low reputation \n",
    "    '''\n",
    "    \n",
    "    # import reputation data\n",
    "    reputation_path = data_path+'reputations/'\n",
    "    reputation_file_name = '%s_first_180_days_eng_reputation.csv'%(community)\n",
    "    rep_data = pd.read_csv(reputation_path+reputation_file_name,index_col=0)\n",
    "    \n",
    "    \n",
    "    # import interactions via questions, answers and comments for network:\n",
    "    interactions_path = data_path+'interactions/'\n",
    "    qa = pd.read_csv(interactions_path + '%s/%s_interactions_questions_answers.csv'%(community,community))\n",
    "    comm = pd.read_csv(interactions_path + '%s/%s_interactions_comments.csv'%(community,community))\n",
    "    acc = pd.read_csv(interactions_path + '%s/%s_interactions_acc_answers.csv'%(community,community))\n",
    "\n",
    "    data = [] # list that will store all data about 30 day windows\n",
    "    for t in range(180-twin):\n",
    "        \n",
    "        new_data_line = [t] \n",
    "        \n",
    "        # interaction slices within [t,t+twin) windows\n",
    "        qna_slice = qa[(qa['days']>=t)&(qa['days']<t+twin)].dropna().copy()#ast had nan's so dropna is included\n",
    "        comm_slice = comm[(comm['days']>=t)&(comm['days']<t+twin)].dropna().copy()\n",
    "        acc_slice = acc[(acc['days']>=t)&(acc['days']<t+twin)].dropna().copy()\n",
    "\n",
    "        # full network of interactions\n",
    "        fullnet = pd.concat([qna_slice[['PostUserId','RespondUserId']], comm_slice[['PostUserId','RespondUserId']], acc_slice[['PostUserId','RespondUserId']]])\n",
    "        fullnet = fullnet[fullnet['PostUserId']!=fullnet['RespondUserId']] #drop selflinks\n",
    "        network = nx.Graph()\n",
    "        network = nx.from_pandas_edgelist(fullnet, source='RespondUserId', target='PostUserId')\n",
    "        network = network.to_undirected()\n",
    "        \n",
    "        # network's mean clustering coef.\n",
    "        clustering_coef = nx.average_clustering(network)\n",
    "        new_data_line.append(clustering_coef)\n",
    "                \n",
    "        # dataframe with users' reputations at t+twin time step\n",
    "        drdf = pd.DataFrame(rep_data[str(t+twin-1)]).reset_index() \n",
    "        drdf = drdf.rename(columns={str(t+twin-1):'dynrep'})\n",
    "             \n",
    "        # slice of dataframe containing only users that are within the interaction network\n",
    "        nodelist = list(network.nodes())\n",
    "        drdf = drdf[drdf['index'].isin(nodelist)]\n",
    "        \n",
    "        # add user's degree and betweenness_centrality to the df with reputations\n",
    "        drdf['degree'] = drdf['index'].map(dict(nx.degree(network)))\n",
    "        drdf['betweenness_centrality'] = drdf['index'].map(nx.betweenness_centrality(network))\n",
    "        \n",
    "        # correlations between node's dynamical reputation and node's degree and betweenness centrality\n",
    "        dr_degree_corr = drdf['dynrep'].corr(drdf['degree'])\n",
    "        dr_bc_corr = drdf['dynrep'].corr(drdf['betweenness_centrality'])\n",
    "        \n",
    "        # to calculate assortativity of dynamical reputation within the interaction network, \n",
    "        # we look at the correlation between dynamical reputations of nodes for each edge in the network\n",
    "        # we construct dataframe where for each post user id, user's reputation is assinged, and similarly for all reposnd user ids\n",
    "        reputations_on_network = pd.merge(left = fullnet, right=drdf,left_on='PostUserId',right_on='index').rename(columns={'dynrep':'PostUserRep'})\n",
    "        reputations_on_network = pd.merge(left=reputations_on_network, right=drdf,left_on='RespondUserId',right_on='index').rename(columns={'dynrep':'RespondUserRep'})\n",
    "\n",
    "        dr_assortativity = reputations_on_network['PostUserRep'].corr(reputations_on_network['RespondUserRep'])\n",
    "        \n",
    "        new_data_line = [t,clustering_coef,dr_degree_corr,dr_bc_corr,dr_assortativity]\n",
    "        data.append(new_data_line)\n",
    "\n",
    "        \n",
    "    data_df = pd.DataFrame(data,columns=['First day','Clustering_coef','DynRep_Degree_corr','DynRep_BC_corr','DynRep_Assortativity'])\n",
    "    \n",
    "    if save_path:    \n",
    "        filename = '%s_dyn_rep_and_networks_features.csv'%(community)\n",
    "        os.makedirs(save_path, exist_ok = True)\n",
    "        data_df.to_csv(save_path+filename,index=False)\n",
    "        \n",
    "    else:    \n",
    "        return(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4976c58",
   "metadata": {},
   "source": [
    "We run this function for all four pairs of communities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7042e250",
   "metadata": {},
   "outputs": [],
   "source": [
    "communities = ['astronomy','economics','literature','physics','052012astronomy','052012economics','052012-literature','052012-theoretical-physics']\n",
    "for comm in communities:\n",
    "    if comm[0].isdigit():\n",
    "        net = 'area51'\n",
    "    else:\n",
    "        net = 'launched'\n",
    "    extract_network_and_dyn_reputation_features(net, comm, data_path+'processed data/',twin=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b02468ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def core_periphery_with_dyn_rep(community,save_path,twin=30):\n",
    "    '''\n",
    "    This function gathers data on 30 day networks core and periphery labels and analyses that data joinly with data about users' dynamical reputation.\n",
    "    Result is a dataframe per community with characteristics of core and periphery for the first 150 30day sliding windows.\n",
    "    Inputs:\n",
    "    - community: name of SE site, used to get appropriate input files that contain reputations and core and periphery nodes and links\n",
    "    - save_path: path to directory where output dataframe should be saved\n",
    "    - twin: time window within which interactions are aggregated, default is 30 days\n",
    "    Outputs:\n",
    "    - if save_path is not provided ('') dataframe is returned, otherwise it is saved\n",
    "    DataFrame contains following columns\n",
    "    - First day of twin sliding window\n",
    "    - N_core - number of users in the network's core\n",
    "    - Frac_core - fraction of users (in %) that belong to the core\n",
    "    - LN_core - links per node within core\n",
    "    - LN_core_periphery - total number of links between core and periphery divided by total number of nodes\n",
    "    - LN_periphery - links per node within periphery\n",
    "    - Mean_dr_core - mean dynamical reputation within core\n",
    "    - DynRep_core_per_ratio - ratio between the total reputation within core and periphery\n",
    "    '''  \n",
    "    \n",
    "    # import reputation data\n",
    "    reputation_path = data_path+'reputations/'\n",
    "    reputation_file_name = '%s_first_180_days_eng_reputation.csv'%(community)\n",
    "    rep_data = pd.read_csv(reputation_path+reputation_file_name,index_col=0)\n",
    "    \n",
    "    # contains data about nodes within core and periphery as well as number of links within and between each group\n",
    "    coreper_path = data_path+'core periphery/'\n",
    "    f = h5py.File(coreper_path+'%s_window%s_core_per.hdf5'%(community,twin), 'r')\n",
    "    \n",
    "    data = [] # list that will store all data about 30 day windows\n",
    "    for t in range(180-twin):\n",
    "        # core and periphery node data\n",
    "        corepernodes = pd.DataFrame(f['labels_%s-%sdays'%(t,t+twin)],columns=['nodes','cat'])\n",
    "        \n",
    "        # data about dynamical reputation for users at the end of the timewindow\n",
    "        drdf = pd.DataFrame(rep_data[str(t+twin-1)]).reset_index()\n",
    "        drdf = drdf.rename(columns={str(t+twin-1):'dynrep'})\n",
    "        \n",
    "        # lists of nodes that belong to core and periphery\n",
    "        corelist = list(corepernodes[corepernodes['cat']==0]['nodes']) # list of nodes within core\n",
    "        perlist = list(corepernodes[corepernodes['cat']==1]['nodes']) # list of nodes within periphery\n",
    "        \n",
    "        Ncore = len(corelist) # size of core\n",
    "        Nper = len(perlist) # size of periphery\n",
    "        fcore = 100*Ncore/(Ncore+Nper) # fraction of nodes that are within core\n",
    "\n",
    "        # links versus nodes ratio within core\n",
    "        links_core_core = f['ms_%s-%sdays'%(t,t+twin)][0,0]\n",
    "        ln_ratio_core = links_core_core/Ncore\n",
    "        \n",
    "        # links versus nodes ratio between core and periphery\n",
    "        links_core_periphery = f['ms_%s-%sdays'%(t,t+twin)][0,1]\n",
    "        ln_ratio_core_periphery = links_core_periphery/(Ncore+Nper)\n",
    "        \n",
    "        # links versus nodes ratio within periphery\n",
    "        links_per_per = f['ms_%s-%sdays'%(t,t+twin)][1,1]\n",
    "        ln_ratio_periphery = links_per_per/Nper\n",
    "\n",
    "        # mean dyn reputation within core\n",
    "        mean_dr_core = drdf[drdf['index'].isin(corelist)].dynrep.mean()\n",
    "        \n",
    "        # ratio between total dynamical reputation within core and periphery \n",
    "        dr_core_periphery = drdf[drdf['index'].isin(corelist)].dynrep.sum()/drdf[drdf['index'].isin(perlist)].dynrep.sum()\n",
    "        \n",
    "        \n",
    "        data.append([t,\n",
    "                     Ncore, # core size measured as total number of nodes\n",
    "                     fcore, # core size measured as % of nodes that belong to core\n",
    "                     ln_ratio_core, # ratio between number of links and nodes within core\n",
    "                     ln_ratio_core_periphery, # ratio between number of links and nodes between core and periphery \n",
    "                     ln_ratio_periphery, # ratio between number of links and nodes within periphery\n",
    "                     mean_dr_core, # mean dyn rep within core\n",
    "                     dr_core_periphery # ratio of dyn rep within core and periphery\n",
    "                    ])\n",
    "\n",
    "    data_df = pd.DataFrame(data,columns=['First day','N_core','Frac_core','LN_core','LN_core_periphery','LN_periphery','Mean_dr_core','DynRep_core_per_ratio'])\n",
    "    \n",
    "    if save_path:    \n",
    "        filename = '%s_dyn_rep_and_core_periphery_features.csv'%(community)\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        data_df.to_csv(save_path+filename, index=False)\n",
    "    else:    \n",
    "        return(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb164cc",
   "metadata": {},
   "source": [
    "We run this function for all four pairs of communities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95c2da99",
   "metadata": {},
   "outputs": [],
   "source": [
    "communities = ['astronomy','economics','literature','physics','052012astronomy','052012economics','052012-literature','052012-theoretical-physics']\n",
    "for comm in communities:\n",
    "    core_periphery_with_dyn_rep(comm, data_path+'processed data/',twin=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e23e69",
   "metadata": {},
   "source": [
    "# data extraction for popular/casual users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd217462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_data_per_user(qna,comm, acc):\n",
    "    user_qs = qna[['QId','PostUserId']].groupby('QId').max().reset_index().groupby('PostUserId').size().to_frame('Q_numb').reset_index()\n",
    "    user_qs = user_qs.rename(columns={'PostUserId':'UserId'})\n",
    "    \n",
    "    user_as = qna.groupby(['QId','RespondUserId']).size().reset_index().groupby('RespondUserId').size().to_frame('A_numb').reset_index()\n",
    "    user_as = user_as.rename(columns = {'RespondUserId':'UserId'})\n",
    "\n",
    "    user_cs = comm.groupby(['QId','PostUserId','RespondUserId']).size().reset_index().groupby('RespondUserId').size().to_frame('C_numb').reset_index()\n",
    "    user_cs = user_cs.rename(columns = {'RespondUserId':'UserId'})\n",
    "    \n",
    "    user_acc = acc.groupby(['QId','PostUserId','RespondUserId']).size().reset_index().groupby('RespondUserId').size().to_frame('ACC_numb').reset_index()\n",
    "    user_acc = user_acc.rename(columns = {'RespondUserId':'UserId'})\n",
    "\n",
    "    #users = pd.merge(pd.merge(user_qs,user_as,on='UserId',how='outer'),user_cs,on='UserId',how='outer')\n",
    "    users = pd.merge(pd.merge(pd.merge(user_qs,user_as,on='UserId',how='outer'),user_cs,on='UserId',how='outer'), user_acc, on='UserId', how='outer')\n",
    "\n",
    "    users['total'] = users['Q_numb'].fillna(0)+users['A_numb'].fillna(0)+users['C_numb'].fillna(0)+users['ACC_numb'].fillna(0)\n",
    "    \n",
    "    return users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62223021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_time_df(data, column_name, t, twin = 30):\n",
    "    condition = (data[column_name]>=t)&(data[column_name]<t+twin)\n",
    "    data_slice = data[condition].dropna().copy()\n",
    "    return data_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb203a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_inactive_user_inetraction(qaT,commT,accT,q):\n",
    "    #extract per user activity from interaction data\n",
    "    users = gather_data_per_user(qaT,commT,accT)\n",
    "\n",
    "    #list of top quantile q users\n",
    "    topus = list(users[users['total']>users['total'].quantile(q)]['UserId']) # top users\n",
    "\n",
    "    #join qna & comm interactions\n",
    "    net_selection  = pd.concat([qaT[['PostUserId','RespondUserId']].dropna(),commT[['PostUserId','RespondUserId']].dropna()])\n",
    "\n",
    "    #network construction\n",
    "    G_full = nx.Graph()\n",
    "    #omit self interactions\n",
    "    G_full = nx.from_pandas_edgelist(net_selection[net_selection['PostUserId']!=net_selection['RespondUserId']], source = 'RespondUserId', target = 'PostUserId')\n",
    "\n",
    "    G_popular = G_full.subgraph(topus)\n",
    "    popular_nodes = len(G_popular.nodes())\n",
    "    popular_edges = len(G_popular.edges())\n",
    "    popular_lpn = popular_edges/popular_nodes\n",
    "    G_casual = G_full.subgraph(set(G_full.nodes())-set(topus))\n",
    "    casual_nodes = len(G_casual.nodes())\n",
    "    casual_edges = len(G_casual.edges())\n",
    "    casual_lpn = casual_edges/casual_nodes\n",
    "    \n",
    "    mix_edges = len(G_full.edges()) - casual_edges - popular_edges\n",
    "    mix_lpn = mix_edges/len(G_full.nodes())\n",
    "\n",
    "    return [popular_nodes,popular_edges,popular_lpn,casual_nodes,casual_edges,casual_lpn,mix_lpn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27896d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def popular_casual_users(community,save_path,q=0.8,twin=30):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    data_address = data_path+'interactions/'\n",
    "    qa = pd.read_csv(data_address+'%s/%s_interactions_questions_answers.csv'%(community,community))\n",
    "    comm = pd.read_csv(data_address+'%s/%s_interactions_comments.csv'%(community,community))\n",
    "    acc = pd.read_csv(data_address + '%s/%s_interactions_acc_answers.csv'%(community,community))\n",
    "\n",
    "    temp_data = []\n",
    "    columns = ['t0','popular_nodes','popular_edges','popular_lpn','casual_nodes','casual_edges','casual_lpn','mix_lpn']\n",
    "    for t in range(180-twin):\n",
    "    #t = 0\n",
    "        qnaT = slice_time_df(qa,'days',t,twin)\n",
    "        commT = slice_time_df(comm,'days',t,twin)\n",
    "        accT = slice_time_df(acc, 'days', t, twin)\n",
    "\n",
    "        temp_data.append([t]+active_inactive_user_inetraction(qnaT,commT,accT,q))\n",
    "    \n",
    "    data_df = pd.DataFrame(temp_data,columns=columns)\n",
    "\n",
    "    if save_path:    \n",
    "        filename = '%s_popular_casual_users.csv'%(community)\n",
    "        os.makedirs(save_path, exist_ok = True)\n",
    "        data_df.to_csv(save_path+filename,index=False)\n",
    "    else:    \n",
    "        return(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dce8c3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "communities = ['astronomy','economics','literature','physics','052012astronomy','052012economics','052012-literature','052012-theoretical-physics']\n",
    "for comm in communities:\n",
    "    popular_casual_users(comm, data_path+'processed data/propopular_users/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341ba23b-31df-41d7-84a1-52142377a56f",
   "metadata": {},
   "source": [
    "## RMSE between number of users in sliding window of 30 days and number of users with reputation higher than 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9ad0046-0604-4b99-927d-37ba6a526dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing_functions import Nusers_sw, prepare_data\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def get_dataframe(community):\n",
    "                  \n",
    "    window = 30\n",
    "    betas = [0.94, 0.941, 0.942, 0.943, 0.944, 0.945, 0.946, 0.947, 0.948, 0.949,\n",
    "             0.95, 0.951, 0.952, 0.953, 0.954, 0.955, 0.956, \n",
    "             0.957, 0.958, 0.959, 0.96, 0.961, 0.962, 0.963, 0.964, \n",
    "             0.965, 0.966, 0.967, 0.968, 0.967, 0.968, 0.969, 0.97]\n",
    "\n",
    "    file_path = data_path+\"reputations/\"\n",
    "\n",
    "    error = []\n",
    "    for b in betas:\n",
    "\n",
    "        #get number of users with reputation higher than 1\n",
    "        file_name = file_path+\"%s_first_180_days_eng_reputation_beta%s.csv\"%(community, b)\n",
    "        dr = pd.read_csv(file_name, index_col=1)\n",
    "\n",
    "        y = (dr>1).sum()\n",
    "        y_actual = np.array(y[window:])\n",
    "\n",
    "        #get number of users in slidng window\n",
    "        interactions_path = data_path+\"interactions/%s/\"%community\n",
    "        U = Nusers_sw(prepare_data( community, 0, 180, 'eng', interactions_path), 180, 30) # data, end time, window\n",
    "        y_predicted = np.array(U[1])\n",
    "\n",
    "        RMSE = math.sqrt(mean_squared_error(y_actual, y_predicted))\n",
    "\n",
    "        error.append((b, RMSE))\n",
    "\n",
    "    dataframe = pd.DataFrame(np.array(error), columns=[\"Beta\", \"RMSE\"])\n",
    "    \n",
    "    \n",
    "    return dataframe\n",
    "    \n",
    "\n",
    "for community in ['physics', 'astronomy', 'economics', 'literature', '052012-theoretical-physics', '052012astronomy', '052012economics', '052012-literature']:\n",
    "    dataframe = get_dataframe(community)\n",
    "    \n",
    "    path = data_path+\"processed data/RMSE/\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    dataframe.to_csv(path+\"rmse_%s.csv\"%community)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e814a33-3f00-4992-aad6-a6b748d407da",
   "metadata": {},
   "source": [
    "## get time interaval between first and last user activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47d5691f-91e4-492c-856f-d696d3f2d1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from data_processing_functions import get_first_last_activity\n",
    "import os\n",
    "\n",
    "def cal_user_time( name, ltlim, htlim):\n",
    "    interaction_path = data_path+'interactions/%s/'%name\n",
    "    df = get_first_last_activity(name, ltlim, htlim, interaction_path)\n",
    "    path = data_path+\"processed data/users_activity/\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    df.to_csv(path+'%s_%s_%s.csv'%(name, ltlim, htlim), index=None)\n",
    "    \n",
    "communities = ['astronomy','economics','literature','physics','052012astronomy','052012economics','052012-literature','052012-theoretical-physics']\n",
    "for comm in communities:\n",
    "    cal_user_time(comm, 0, 180)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f541b8a6-d2d4-4aa9-b142-d0e96b0097d3",
   "metadata": {},
   "source": [
    "## Minimum description length, number of nodes in core, normalized mutual information, adjusted rand index, F1 measure and Jaccard index, among 50 samples for 30-days sub-networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "582a46d0-5a50-46fb-a421-2ed38d4e3826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "def get_labels(ltlim, htlim, data):\n",
    "    #data = h5py.File('results_ensemble_test/052012astronomy/052012astronomy_%s-%sdays.hdf5'%(ltlim, htlim), 'r')\n",
    "    total_core = {}\n",
    "    total_mdl = {}\n",
    "    for j in range(0, 50):\n",
    "\n",
    "        labels = np.array(data['labels_%s'%j])\n",
    "        mdl  = np.array(data['mdl_%s'%j])\n",
    "                \n",
    "        total_core[j] = labels\n",
    "        total_mdl[j] = mdl\n",
    "        \n",
    "    return total_core, total_mdl\n",
    "\n",
    "def select_core(ls):\n",
    "    core = []\n",
    "    for i in ls:\n",
    "        if i[1]==0.:\n",
    "            core.append(i[0])\n",
    "    return core\n",
    "\n",
    "import os\n",
    "\n",
    "name = '052012astronomy'\n",
    "twin=30\n",
    "\n",
    "adj_time = {}\n",
    "nmi_time = {}\n",
    "f1_time = {}\n",
    "jacc_time = {}\n",
    "\n",
    "for i in range(180-twin):\n",
    "    ltlim = i\n",
    "    htlim = i + twin\n",
    "    path = data_path+'core periphery/core-periphery_ens/052012astronomy/'\n",
    "    data = h5py.File(path+'%s_%s-%sdays.hdf5'%(name, ltlim, htlim), 'r')\n",
    "\n",
    "    labels_sample, mdl_sample = get_labels(ltlim,htlim, data)\n",
    "\n",
    "    adj = []\n",
    "    nmi = []\n",
    "    f1 = []\n",
    "    jacc = []\n",
    "    for itt1 in range(len(labels_sample)):\n",
    "\n",
    "        for itt2 in range(itt1, len(labels_sample)-1):\n",
    "\n",
    "            if itt1!=itt2:\n",
    "\n",
    "                sample1 = [ int(i[1]) for i in labels_sample[itt1]]\n",
    "                sample2 = [ int(i[1]) for i in labels_sample[itt2]]\n",
    "\n",
    "                adj.append(adjusted_rand_score(sample1, sample2))\n",
    "                nmi.append(normalized_mutual_info_score(sample1, sample2))\n",
    "                f1.append(f1_score(sample1, sample2, average='macro'))\n",
    "                jacc.append(jaccard_score(sample1, sample2, average='macro'))\n",
    "                         \n",
    "    adj_time[i] = adj\n",
    "    nmi_time[i] = nmi\n",
    "    f1_time[i] = f1\n",
    "    jacc_time[i] = jacc\n",
    "\n",
    "\n",
    "    \n",
    "res_path = data_path+\"processed data/core-periphery_ens_statistics/\"\n",
    "os.makedirs(res_path, exist_ok=True)\n",
    "pd.DataFrame(adj_time).to_csv(res_path+'/%s_ari.csv'%( name))\n",
    "pd.DataFrame(nmi_time).to_csv(res_path+'%s_nmi.csv'%( name))\n",
    "pd.DataFrame(f1_time).to_csv(res_path+'/%s_f1.csv'%( name))\n",
    "pd.DataFrame(jacc_time).to_csv(res_path+'%s_jacc.csv'%( name))\n",
    "\n",
    "\n",
    "\n",
    "mdl_time = {}\n",
    "node_time = {}\n",
    "\n",
    "for i in range(180-twin):\n",
    "    ltlim = i\n",
    "    htlim = i + twin\n",
    "    path = data_path+'core periphery/core-periphery_ens/052012astronomy/'\n",
    "    data = h5py.File(path+'/%s_%s-%sdays.hdf5'%(name, ltlim, htlim), 'r')\n",
    "    labels_sample, mdl_sample = get_labels(ltlim,htlim, data)\n",
    "    mdl = []\n",
    "    nodes = []\n",
    "    for key, val in mdl_sample.items():\n",
    "        mdl.append(float(val))\n",
    "    mdl_time[i] = mdl\n",
    "    \n",
    "    for _, lab in labels_sample.items():\n",
    "        \n",
    "        core = select_core(lab)\n",
    "        nodes.append(len(core))\n",
    "            \n",
    "    node_time[i]= nodes\n",
    "    \n",
    "pd.DataFrame(node_time).to_csv(res_path+'/%s_nodes_in_core.csv'%(name))\n",
    "pd.DataFrame(mdl_time).to_csv(res_path+'/%s_mdl.csv'%(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed999ff-d6df-4ac1-9ab7-ac90220cd730",
   "metadata": {},
   "source": [
    "## jaccard index between core users in sub-networks at time points t1 and t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46b72370-f27f-4332-aa52-d35b35a90c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import h5py\n",
    "import numpy as np\n",
    "def import_core(name, steps):\n",
    "    \n",
    "    fname = data_path+'core periphery/%s_window30_core_per.hdf5'%(name)\n",
    "    time0=0\n",
    "    time1=30\n",
    "    data = []\n",
    "    with h5py.File(fname, 'r') as W:\n",
    "        for r in range(steps):\n",
    "            ltlim = time0 + r\n",
    "            htlim = time1 + r\n",
    "           \n",
    "            if 'ns_%s-%sdays'%(ltlim, htlim) in W.keys():\n",
    "                labels = np.array(W['labels_%s-%sdays'%(ltlim, htlim)]) #= np.array(results[r][1])\n",
    "                data.append(labels)\n",
    "    return data\n",
    "\n",
    "#define Jaccard Similarity function\n",
    "def jaccard(list1, list2):\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(list1) + len(list2)) - intersection\n",
    "    return float(intersection) / union\n",
    "\n",
    "def calculate_jaccard(labels):\n",
    "    #steps= 150\n",
    "    twin=30\n",
    "\n",
    "    core_list = []\n",
    "    for i in labels:\n",
    "        core_list.append( [ line[0] for line in i if line[1]==0])\n",
    "\n",
    "    Z = []\n",
    "    for i in range(0, 180-twin):\n",
    "        for j in range(0, 180-twin):\n",
    "            Z.append((i, j, jaccard(core_list[i], core_list[j])))\n",
    "            \n",
    "    return Z\n",
    "\n",
    "\n",
    "\n",
    "def dataframe(name):\n",
    "    twin=30\n",
    "    labels = import_core(name, 180-twin)\n",
    "    Z = calculate_jaccard(labels)\n",
    "    lag_j = []\n",
    "    for i in Z:\n",
    "        lag_j.append( (np.abs( i[1]-i[0]), i[2]))\n",
    "    df = pd.DataFrame(lag_j, columns=['time-delta', 'jaccard'])\n",
    "    df['name'] = name\n",
    "    return df\n",
    "\n",
    "for community in [\"astronomy\", \"052012astronomy\", \n",
    "                 \"economics\", \"052012economics\", \n",
    "                 \"physics\", \"052012-theoretical-physics\", \n",
    "                 \"literature\", \"052012-literature\"]:\n",
    "    \n",
    "    df = dataframe(community)\n",
    "    path = data_path+\"processed data/jaccard/\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    df.to_csv(path+\"%s_jaccard_time_delta.csv\"%community)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139c68b7-dcdd-4a0d-bf0c-bc3cf89d744d",
   "metadata": {},
   "source": [
    "## Jaccard index between core users in sub-networks at time points t1 and t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c998f4f-a1b6-4ba0-ad4c-112fbebf9556",
   "metadata": {},
   "outputs": [],
   "source": [
    "for community in ['052012astronomy',  '052012economics',  '052012-literature',  '052012-theoretical-physics',\n",
    "             'astronomy', 'economics', 'literature',   'physics',  \n",
    "            ]:\n",
    "    twin=30\n",
    "    labels = import_core(community, 180-twin)\n",
    "\n",
    "    core_list = []\n",
    "    for i in labels:\n",
    "        core_list.append( [ line[0] for line in i if line[1]==0])\n",
    "\n",
    "    Z = []\n",
    "    for i in range(0, 180-twin):\n",
    "        for j in range(0, 180-twin):\n",
    "            Z.append((i, j, jaccard(core_list[i], core_list[j])))\n",
    "\n",
    "    Z = np.array(Z)\n",
    "    df = pd.DataFrame(Z, columns = ['t1','t2', 'J'])\n",
    "    path = data_path+\"processed data/jaccard/\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    df.to_csv(path+\"%s_jaccard.csv\"%community)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3202d1c1-a0a1-4a23-a467-e45d0ac32d4e",
   "metadata": {},
   "source": [
    "## network and core-periphery properties for different sliding windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1fc4794c-61be-4e47-a8aa-f35e7c21435b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import h5py\n",
    "\n",
    "def calculate_fig14_csv(community, twin=30):\n",
    "    \n",
    "    data = {\"First_day\":[],\n",
    "            \"N_nodes\":[],\n",
    "            \"L/N\":[],\n",
    "            \"clustering\":[],\n",
    "            \n",
    "            \"L/N_core\": [],\n",
    "            \"N_core\": [],\n",
    "            \"L/N_core_per\":[],\n",
    "            \"mean_core_dr\":[],\n",
    "            \"mean_core/per_dr\":[],\n",
    "    }\n",
    "    \n",
    "    reputation_path = data_path+'reputations/'\n",
    "    reputation_file_name = '%s_first_180_days_eng_reputation.csv'%(community)\n",
    "    rep_data = pd.read_csv(reputation_path+reputation_file_name,index_col=0)\n",
    "    \n",
    "    # import interactions via questions, answers and comments for network:\n",
    "    interactions_path = data_path+'interactions/'\n",
    "    qa = pd.read_csv(interactions_path + '%s/%s_interactions_questions_answers.csv'%(community,community))\n",
    "    comm = pd.read_csv(interactions_path + '%s/%s_interactions_comments.csv'%(community,community))\n",
    "    acc = pd.read_csv(interactions_path + '%s/%s_interactions_acc_answers.csv'%(community,community))\n",
    "    \n",
    "    # contains data about nodes within core and periphery as well as number of links within and between each group\n",
    "    coreper_path = data_path+'core periphery/'\n",
    "    f = h5py.File(coreper_path+'%s_window%s_core_per.hdf5'%(community, twin), 'r')\n",
    "    \n",
    "    for t in range(180-twin):\n",
    "        \n",
    "        qna_slice = qa[(qa['days']>=t)&(qa['days']<t+twin)].dropna().copy()#ast had nan's so dropna is included\n",
    "        comm_slice = comm[(comm['days']>=t)&(comm['days']<t+twin)].dropna().copy()\n",
    "        acc_slice = acc[(acc['days']>=t)&(acc['days']<t+twin)].dropna().copy()\n",
    "\n",
    "        # full network of interactions\n",
    "        fullnet = pd.concat([qna_slice[['PostUserId','RespondUserId']], comm_slice[['PostUserId','RespondUserId']], acc_slice[['PostUserId','RespondUserId']]])\n",
    "        fullnet = fullnet[fullnet['PostUserId']!=fullnet['RespondUserId']] #drop selflinks\n",
    "        network = nx.Graph()\n",
    "        network = nx.from_pandas_edgelist(fullnet, source='RespondUserId', target='PostUserId')\n",
    "        G = network.to_undirected()\n",
    "        \n",
    "        data[\"First_day\"].append(t)\n",
    "        \n",
    "        N = G.number_of_nodes()\n",
    "        L = G.number_of_edges()\n",
    "        if N!=0:\n",
    "            dens = L/N\n",
    "            c = nx.average_clustering(G)\n",
    "        else:\n",
    "            c = 0\n",
    "            dens = 0\n",
    "            \n",
    "        data[\"N_nodes\"].append(N)\n",
    "        data[\"L/N\"].append(dens)\n",
    "        data[\"clustering\"].append(c)\n",
    "        \n",
    "        \n",
    "        # core and periphery node data\n",
    "        corepernodes = pd.DataFrame(f['labels_%s-%sdays'%(t,t+twin)],columns=['nodes','cat'])\n",
    "        \n",
    "        # data about dynamical reputation for users at the end of the timewindow\n",
    "        drdf = pd.DataFrame(rep_data[str(t+twin-1)]).reset_index()\n",
    "        drdf = drdf.rename(columns={str(t+twin-1):'dynrep'})\n",
    "        \n",
    "        # lists of nodes that belong to core and periphery\n",
    "        corelist = list(corepernodes[corepernodes['cat']==0]['nodes']) # list of nodes within core\n",
    "        perlist = list(corepernodes[corepernodes['cat']==1]['nodes']) # list of nodes within periphery\n",
    "        \n",
    "       \n",
    "        \n",
    "        Ncore = len(corelist) # size of core\n",
    "        Nper = len(perlist) # size of periphery\n",
    "        \n",
    "        if Ncore==0:\n",
    "            fcore =0\n",
    "            ln_ratio_core=0\n",
    "        else:\n",
    "            fcore = 100*Ncore/(Ncore+Nper) # fraction of nodes that are within core\n",
    "\n",
    "            # links versus nodes ratio within core\n",
    "        \n",
    "            \n",
    "            links_core_core = f['ms_%s-%sdays'%(t,t+twin)][0,0]\n",
    "            ln_ratio_core = links_core_core/Ncore\n",
    "            \n",
    "        \n",
    "        # links versus nodes ratio between core and periphery\n",
    "        links_core_periphery = f['ms_%s-%sdays'%(t,t+twin)][0,1]\n",
    "        ln_ratio_core_periphery = links_core_periphery/(Ncore+Nper)\n",
    "        \n",
    "        # links versus nodes ratio within periphery\n",
    "        links_per_per = f['ms_%s-%sdays'%(t,t+twin)][1,1]\n",
    "        ln_ratio_periphery = links_per_per/Nper\n",
    "\n",
    "        # mean dyn reputation within core\n",
    "        mean_dr_core = drdf[drdf['index'].isin(corelist)].dynrep.mean()\n",
    "        \n",
    "        # ratio between total dynamical reputation within core and periphery \n",
    "        dr_core_periphery = drdf[drdf['index'].isin(corelist)].dynrep.mean()/drdf[drdf['index'].isin(perlist)].dynrep.mean()\n",
    "        \n",
    "        \n",
    "        data[\"N_core\"].append(Ncore)\n",
    "        data[\"L/N_core\"].append(ln_ratio_core)\n",
    "        data[\"L/N_core_per\"].append(ln_ratio_core_periphery)\n",
    "        data[\"mean_core_dr\"].append(mean_dr_core)\n",
    "        data[\"mean_core/per_dr\"].append(dr_core_periphery)\n",
    "        \n",
    "        \n",
    "    return data\n",
    "\n",
    "import os\n",
    "for window in [10, 30, 60]:\n",
    "    \n",
    "    for community in [\"astronomy\", \"052012astronomy\"]:\n",
    "        data = calculate_fig14_csv(community, twin=window)\n",
    "        df = pd.DataFrame(data)\n",
    "        path = data_path+\"processed data/network_properties_window/\"\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        \n",
    "        file_name = path+\"%s_network_properties_window%s.csv\"%(community, window)\n",
    "        df.to_csv(file_name, index=None)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0acbdb7-15a4-49d6-80fc-2e91cbe54306",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
